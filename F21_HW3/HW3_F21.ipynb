{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNnrjkIUWEDh"
   },
   "source": [
    "# Homework 3\n",
    "\n",
    "DUE Nov 15th at 11:59 PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbpTqmT4WhaB"
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "In this problem, you will implement a simple feed-forward neural network using PyTorch, a straight-forward and simple-to-pickup framework for quickly prototyping deep learning model. \n",
    "\n",
    "PyTorch provides 2 powerful things. First, a nice data structure called Tensor (basically a matrix, similar to Numpy ndarray). Tensor is optimized for matrix calculation and can be loaded to a GPU. Tensor is also implemented so that it's easy to calculate and pass back chains of gradients, which is extremely useful for backpropagation on neural network. Second, a nice inner mechanism called Autograd that nicely maps variables involved a chain of calculations and efficiently calculates their gradients via the chain rule when needed. Read more here: https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95  \n",
    "\n",
    "You will define a neural network class in PyTorch and use the network to learn a classification task on the famous KDD CUP 99 dataset. You can refer to Problem 2 to see how a network class can be defined, how to use a PyTorch's DataLoader, and how a training loop may looks like.\n",
    "\n",
    "There are many greate tutorial on PyTorch out there. For example, this video on Youtube explains how to build a simple network in PyTorch quite clearly: https://www.youtube.com/watch?v=oPhxf2fXHkQ\n",
    "\n",
    "### Part a\n",
    "Firstly, load and inspect the \"**KDD CUP 99**\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qyL0okEBWggC"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "X, y = fetch_kddcup99(return_X_y=True, percent10=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>235</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>219</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494016</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>310</td>\n",
       "      <td>1881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494017</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>282</td>\n",
       "      <td>2286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494018</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>203</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494019</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>291</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494020</th>\n",
       "      <td>0</td>\n",
       "      <td>b'tcp'</td>\n",
       "      <td>b'http'</td>\n",
       "      <td>b'SF'</td>\n",
       "      <td>219</td>\n",
       "      <td>1234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>255</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>494021 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1        2      3    4     5  6  7  8  9   ...  31   32   33  \\\n",
       "0       0  b'tcp'  b'http'  b'SF'  181  5450  0  0  0  0  ...   9    9  1.0   \n",
       "1       0  b'tcp'  b'http'  b'SF'  239   486  0  0  0  0  ...  19   19  1.0   \n",
       "2       0  b'tcp'  b'http'  b'SF'  235  1337  0  0  0  0  ...  29   29  1.0   \n",
       "3       0  b'tcp'  b'http'  b'SF'  219  1337  0  0  0  0  ...  39   39  1.0   \n",
       "4       0  b'tcp'  b'http'  b'SF'  217  2032  0  0  0  0  ...  49   49  1.0   \n",
       "...    ..     ...      ...    ...  ...   ... .. .. .. ..  ...  ..  ...  ...   \n",
       "494016  0  b'tcp'  b'http'  b'SF'  310  1881  0  0  0  0  ...  86  255  1.0   \n",
       "494017  0  b'tcp'  b'http'  b'SF'  282  2286  0  0  0  0  ...   6  255  1.0   \n",
       "494018  0  b'tcp'  b'http'  b'SF'  203  1200  0  0  0  0  ...  16  255  1.0   \n",
       "494019  0  b'tcp'  b'http'  b'SF'  291  1200  0  0  0  0  ...  26  255  1.0   \n",
       "494020  0  b'tcp'  b'http'  b'SF'  219  1234  0  0  0  0  ...   6  255  1.0   \n",
       "\n",
       "         34    35    36    37    38   39   40  \n",
       "0       0.0  0.11   0.0   0.0   0.0  0.0  0.0  \n",
       "1       0.0  0.05   0.0   0.0   0.0  0.0  0.0  \n",
       "2       0.0  0.03   0.0   0.0   0.0  0.0  0.0  \n",
       "3       0.0  0.03   0.0   0.0   0.0  0.0  0.0  \n",
       "4       0.0  0.02   0.0   0.0   0.0  0.0  0.0  \n",
       "...     ...   ...   ...   ...   ...  ...  ...  \n",
       "494016  0.0  0.01  0.05   0.0  0.01  0.0  0.0  \n",
       "494017  0.0  0.17  0.05   0.0  0.01  0.0  0.0  \n",
       "494018  0.0  0.06  0.05  0.06  0.01  0.0  0.0  \n",
       "494019  0.0  0.04  0.05  0.04  0.01  0.0  0.0  \n",
       "494020  0.0  0.17  0.05   0.0  0.01  0.0  0.0  \n",
       "\n",
       "[494021 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the X dataset to a dataframe and factorize the columns\n",
    "X_pd = pd.DataFrame(X)\n",
    "X_pd['class'] = y\n",
    "X_pd.iloc[:,1] = pd.factorize(X_pd.iloc[:,1])[0]\n",
    "X_pd.iloc[:,2] = pd.factorize(X_pd.iloc[:,2])[0]\n",
    "X_pd.iloc[:,3] = pd.factorize(X_pd.iloc[:,3])[0]\n",
    "X_pd.loc[:,'class'] = pd.factorize(X_pd.loc[:,'class'])[0]\n",
    "\n",
    "# separate y as its own variable\n",
    "use_y = np.array(X_pd.loc[:,'class'])\n",
    "\n",
    "# drop the class variable from the dataset\n",
    "X_pd = X_pd.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0   1   2   3    4     5  6  7  8  9   ...  31  32   33   34    35   36  \\\n",
       "0  0   0   0   0  181  5450  0  0  0  0  ...   9   9  1.0  0.0  0.11  0.0   \n",
       "1  0   0   0   0  239   486  0  0  0  0  ...  19  19  1.0  0.0  0.05  0.0   \n",
       "2  0   0   0   0  235  1337  0  0  0  0  ...  29  29  1.0  0.0  0.03  0.0   \n",
       "3  0   0   0   0  219  1337  0  0  0  0  ...  39  39  1.0  0.0  0.03  0.0   \n",
       "4  0   0   0   0  217  2032  0  0  0  0  ...  49  49  1.0  0.0  0.02  0.0   \n",
       "\n",
       "    37   38   39   40  \n",
       "0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.  , 0.  , 0.  ],\n",
       "       ...,\n",
       "       [0.  , 0.  , 0.  , ..., 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.01, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , ..., 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the X_pd dataframe to an array again\n",
    "X_use = np.array(X_pd.astype('float64'))\n",
    "X_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD09sAm-52_u"
   },
   "source": [
    "Split them into a train set (70%), a validation set (10%), and a test set (20%). Then, create a PyTorch's DataLoader for the train set, a DataLoader for the validation set, and a DataLoader for the test set.\n",
    "\n",
    "You can read about PyTorch's DataLoader from:\n",
    "\n",
    "*   https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "*   https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used to create a pytorch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, X, y):\n",
    "        'Initialization'\n",
    "        self.labels = torch.tensor(y)\n",
    "        self.features = torch.tensor(X).float()\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "\n",
    "        return self.features[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tnwd2ylj5BD4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# split using train_test_split\n",
    "X_train, X_test_full, y_train, y_test_full = train_test_split(X_use, use_y, test_size=0.3, random_state=674)\n",
    "X_test, X_validate, y_test, y_validate = train_test_split(X_test_full, y_test_full, test_size=(1/3), random_state=638)\n",
    "\n",
    "# create the pytorch datasets using the newly created function above\n",
    "train_dataset = Dataset(X_train, y_train)\n",
    "test_dataset = Dataset(X_test, y_test)\n",
    "valid_dataset = Dataset(X_validate, y_validate)\n",
    "\n",
    "# create the loaders for each of the sets\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = 64, shuffle = False)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset = valid_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  tensor([[ 0.,  2.,  9.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 11.,  ...,  1.,  0.,  0.],\n",
      "        [ 0.,  0., 11.,  ...,  1.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  1.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  2.,  9.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0., 11.,  ...,  1.,  0.,  0.]])\n",
      "y:  tensor([ 5,  4,  4,  4,  0,  0,  0, 10,  5,  4,  5,  0,  5,  0,  5,  5,  4,  5,\n",
      "         0,  8,  5,  5,  4,  5,  0,  4,  5,  5,  5,  4,  4,  5,  4,  5,  5,  5,\n",
      "         5,  4,  0,  4,  5,  4,  5,  5,  5,  4,  4,  5,  4,  5,  5,  0,  5,  5,\n",
      "         5,  5,  4,  5,  5,  4,  5,  0,  5,  4])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(\"X: \", data)\n",
    "    print(\"y: \", target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345814, 41)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZuEYUkVdWpYw"
   },
   "source": [
    "### Part b \n",
    "Create a Python class for our neural network model. The network should have 1 input layer, 1 hidden layer, and 1 output layer. You are free to choose the size of the hidden layer (it may affect the performance). Use ReLU as the activation function (torch.relu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "68ti8_83WzHP"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Any Pytorch's network class is an extension of the torch.nn.Module parent class.\n",
    "# To define a network class, you need to define at least 2 methods: an __init__() method (constructor) and a forward() method\n",
    "class SimpleNetwork(torch.nn.Module):\n",
    "    # Create the network class by filling in this block of code\n",
    "\n",
    "    # Create the constructor. Add any additional arguments as you wish\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    # Define the feed forward function.\n",
    "    # x is the input example/examples.\n",
    "    # Add any additional arguments as you wish.\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONBn93ldWz8n"
   },
   "source": [
    "### Part c \n",
    "Train the network using the training dataset. Use the SGD optimizer and CrossEntropyLoss. After each epoch, record the current loss and the current training accuracy. The current training accuracy is obtained by evaluating the model on the train dataset. Use the DataLoaders defined in part a to efficiently pass training and testing data.\n",
    "\n",
    "You can learn about the available optimizers at:\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "You can learn about the available loss functions at:\n",
    "https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 41]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8zDhPrRYW4ho",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "input_size = 41\n",
    "hidden_size = 100\n",
    "num_classes = 23\n",
    "\n",
    "# initialize the model before the loop\n",
    "model = SimpleNetwork(input_size, hidden_size,num_classes)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "valid_scores =[]\n",
    "losses = []\n",
    "train_scores = []\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # 100, 41\n",
    "    # 100\n",
    "    print('Epoch: '+str(epoch))\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "        curr_features = features\n",
    "        curr_labels = labels\n",
    "        # forward\n",
    "        outputs = model(curr_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # backwards\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i== 0:\n",
    "            # obtain train accuracy\n",
    "            with torch.no_grad():\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for X, y in train_loader:\n",
    "                    outputs = model(X.float())\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    total += y.shape[0]\n",
    "                    correct += (predictions == y).sum().item()\n",
    "            train_scores.append(correct/total)\n",
    "            # obtain validation score\n",
    "            with torch.no_grad():\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for X, y in valid_loader:\n",
    "                    outputs = model(X.float())\n",
    "                    _, predictions = torch.max(outputs, 1)\n",
    "                    total += y.shape[0]\n",
    "                    correct += (predictions == y).sum().item()\n",
    "            valid_scores.append(correct/total)\n",
    "            # obtain the losses\n",
    "            losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j31M2YA4W-yk"
   },
   "source": [
    "Plot how the loss and the training accuracy and the validation accuracy change over the epochs. Is there a point where overfitting occurs? If you cannot spot one, answer no. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "EiAD2opxW_gY"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lines\n",
    "plt.plot(range(1,EPOCHS + 1), valid_scores, label = \"Validation Scores\")\n",
    "plt.plot(range(1,EPOCHS  + 1), train_scores, label = \"Training Scores\")\n",
    "z = np.polyfit(range(1,EPOCHS + 1), train_scores, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(range(1,EPOCHS + 1), p(range(1,EPOCHS + 1)), label = 'Accuracy Trend Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb1b09bfa90>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,EPOCHS  + 1), losses, label = \"Losses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_Based on the output of the first plot, it looks like the training and validation accuracy increases (very slightly) as the number of epochs increases. We can see this with the trend line on the graph. With the loss plotted against the number of epochs, we see the opposite is true and that it overall decreases as the number of epochs increases. This is because the first loss is extremely high while the following losses are relatively low. Since the the first loss is computed without any training, then it is going to be high at first. Since the Validation Accuracy follows pretty close to the Training Accuracy, there seems to be no overfitting happening. For overfitting, I would expect that the training accuracy would be much higher than the validation accuracy._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCRxAtjnXB55"
   },
   "source": [
    "### Part d \n",
    "Evaluate the model on the test dataset. Print out the accuracy. Does this accuracy agree with the training accuracy showed on the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uGAvfpWuXHLM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79949/98804 = 0.809167645034614\n"
     ]
    }
   ],
   "source": [
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        test_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "print('Test Accuracy: {}/{} = {}'.format(correct,len(test_loader.dataset),correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_Based on what the plot from part c shows, the testing accuracy matches what we'd expect after training and validating the model on seen data and exposing it to unseen data. It looks like the max accuracy of the training and validating gets to be close to 84% while the test accuracy is about an 81%._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnF9NTStY73w"
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "In this problem, we will investigate the effects of various common hyperparameters on the performance of a neural network. In the following cell, you can find a network class already defined for you. You can initiate network instances with different hyperparameters by changing the contructor's arguments.\n",
    "\n",
    "You are graded based on how you implement and execute the experiments. Since there is some randomness in initiating and training a neural network, there is no guarantee that you will get an expected result for an experiment or that your results should be similar to those of your peers. The expected outcome is that you execute the experiments correctly and the conclusion you get are consistent with your results. For each experiments, try to run the code multiple times and record the average results like what we did in Homework 2 (it will take some time to run, as expected when training any neural network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 26623,
     "status": "ok",
     "timestamp": 1636073089521,
     "user": {
      "displayName": "Kha-Dinh Luong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12651085651574501268"
     },
     "user_tz": 420
    },
    "id": "Gomk6vh0ZjQJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class mnist_network(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_hidden_layers=1, layer_size = 100, activation=None):\n",
    "        super(mnist_network, self).__init__()\n",
    "        # layers of the network\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = activation\n",
    "\n",
    "        if(self.activation == 'relu'):\n",
    "            self.activation = F.relu\n",
    "        elif(self.activation == 'tanh'):\n",
    "            self.activation = torch.tanh\n",
    "        elif(self.activation == 'sigmoid'): # added this line\n",
    "            self.activation = torch.nn.Sigmoid # and this one\n",
    "\n",
    "        self.layers = nn.ModuleList([nn.Linear(784,self.layer_size)])\n",
    "        for i in range(1, self.num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(self.layer_size,self.layer_size))\n",
    "        self.layers.append(nn.Linear(self.layer_size,10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # converting each image into a vector\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size,-1)\n",
    "        # rest of the forward pass \n",
    "        for i in range(self.num_hidden_layers+1):\n",
    "            x = self.layers[i](x)\n",
    "            if(self.activation != None):\n",
    "                x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0DVPRosZkSF"
   },
   "source": [
    "Run the following code to load the MNIST dataset. For the sake of simplicity, we do not have a validation set in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eJ8_gdWvZtpU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "\n",
    "# DataLoader is a nice tool provided by PyTorch for passing training or testing examples\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCTzQwi5GdRk"
   },
   "source": [
    "Here is an example of training and testing a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, train_loader, optimizer, epoch):\n",
    "    # Turn the model to training mode (gradients will be calculated)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # We have to call zero_grad() on the optimizer to remove gradients from the previous data pass.\n",
    "        # Otherwise, the gradients will be accumulated throughout many passes.\n",
    "        optimizer.zero_grad()\n",
    "        # Pass in the data and obtain the output.\n",
    "        # When you pass the data directly by calling model(data), the model will internally pass the data through the forward() function.\n",
    "        output = model(data)\n",
    "        # Compare the output and the ground truth and calculate the loss.\n",
    "        loss = criterion(output, target)\n",
    "        # From the calculated loss, call backward() to calculate the gradients for all the paramters in the network.\n",
    "        loss.backward()\n",
    "        # Update the parameters according to the gradients. \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, criterion, test_loader):\n",
    "    # Turn the model to testing mode (gradients will not be calculated)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TaSecQQAoDHb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303005\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.698578\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.094308\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.884926\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.659386\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.872297\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.788716\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.121195\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.850268\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.800012\n",
      "\n",
      "Test set: Average loss: 0.7488, Accuracy: 8150/10000 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.686632\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.896054\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.701346\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.654898\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.511969\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.770084\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.654108\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.064980\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.758033\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.776658\n",
      "\n",
      "Test set: Average loss: 0.6969, Accuracy: 8358/10000 (84%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.601664\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.858552\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.669929\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.628938\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.486381\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.744605\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.607090\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.050578\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.709480\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.763184\n",
      "\n",
      "Test set: Average loss: 0.6735, Accuracy: 8439/10000 (84%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.563237\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.840385\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.649820\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.622158\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.472149\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.731105\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.578710\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.044989\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.672262\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.752988\n",
      "\n",
      "Test set: Average loss: 0.6580, Accuracy: 8495/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.540288\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.829955\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.632253\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.615760\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.460427\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.719078\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.556739\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.039775\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.644610\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.741415\n",
      "\n",
      "Test set: Average loss: 0.6460, Accuracy: 8520/10000 (85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "epochs = 5\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Create the model\n",
    "model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# Define the training and testing loss\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Define the optimizer\n",
    "# We have to specify the learning rate and the parameters that the optimizer should update during training.\n",
    "# In this case, we specify that the optimizer should update all the parameters from our model.\n",
    "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "for epoch in range(1,1+epochs):\n",
    "        # Training\n",
    "        train(model, train_criterion, train_loader, optimizer, epoch)\n",
    "        # Testing\n",
    "        test(model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnm5pgiLhBmM"
   },
   "source": [
    "## Part a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spGF1F_NZvt1"
   },
   "source": [
    "First, we will investigate the effect of varying the size of the hidden layer. Create 3 one-hidden-layer networks with the sizes of the hidden layers being 5, 20, 50, respectively. We will call these the 5-network, the 20-network, and the 50-network. All networks should use ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**will create the networks within their respective cells so when I rerun it wont be trained already**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLDj0aMPaOP6"
   },
   "source": [
    "Train the 5-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rFtFPtnOayqM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.382977\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.190428\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.112933\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.048495\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.018739\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.933065\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.708500\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.998135\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.746063\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.462017\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.618658\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.477855\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.463898\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.374288\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.372230\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.418761\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.250929\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.524912\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.372139\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.075894\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.239668\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.199927\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.198971\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.137233\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.148968\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.231528\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.079029\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.305544\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.198306\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.912956\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.080326\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.067518\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.047245\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.023566\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.041417\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.125970\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.970966\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.176769\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.088650\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.823190\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.979611\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.984689\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.958984\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.954720\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.979728\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.056434\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.899459\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.098208\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.013956\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.769626\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.908803\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.927161\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.897376\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.904946\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.936860\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.004389\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.848875\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.041081\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.961641\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.734429\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.858685\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.885018\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.855066\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.867420\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.907437\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.964595\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.811154\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.998472\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.920698\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.711223\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.818416\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.852030\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.821297\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.836839\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.886339\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.933151\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.778486\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.963969\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.887651\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.689923\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.785332\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.825106\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.796276\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.812063\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.869828\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.901324\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.751277\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.934787\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.861029\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.671161\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.756000\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.797829\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.775542\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.790168\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.857445\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.875399\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.727807\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.909144\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.837476\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.655269\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Define the training and testing loss\n",
    "train_criterion = nn.CrossEntropyLoss()\n",
    "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# Define the optimizer\n",
    "# We have to specify the learning rate and the parameters that the optimizer should update during training.\n",
    "# In this case, we specify that the optimizer should update all the parameters from our model.\n",
    "\n",
    "# define the five-network\n",
    "five_model = mnist_network(num_hidden_layers=1,layer_size=5,activation='relu')\n",
    "\n",
    "# create optimizer for the 5-network\n",
    "five_optimizer = optim.SGD(five_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "five_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the five-network\n",
    "    train(five_model, train_criterion, train_loader, five_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    five_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = five_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    five_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5118833333333334, 0.6109166666666667, 0.6497833333333334, 0.6712, 0.6860166666666667, 0.6977166666666667, 0.7065333333333333, 0.7143333333333334, 0.7206, 0.7703833333333333]\n"
     ]
    }
   ],
   "source": [
    "print(five_model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShqrwsCjazpB"
   },
   "source": [
    "Test the trained 5-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DUbHZYLxa5yw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.8030, Accuracy: 7802/10000 (78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(46)\n",
    "test(five_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b39B1xD9bhez"
   },
   "source": [
    "Train the 20-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yKUZhq6lbfvB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.332318\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.165433\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.205512\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.964749\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.001285\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.864156\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.650214\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.676998\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.470283\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.306746\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.254378\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.059246\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.177335\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.937805\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.896074\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.853867\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.754357\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.939809\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.882449\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.756683\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.715236\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.599413\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.707172\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.616923\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.558279\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.583628\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.504871\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.696237\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.694062\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.589533\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.535757\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.452891\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.536312\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.505134\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.436178\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.491091\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.406989\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.591226\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.598525\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.520997\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.448198\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.384843\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.452579\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.452524\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.373257\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.446068\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.354190\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.533137\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.537044\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.486789\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.395762\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.346953\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.401971\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.422501\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.334905\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.419072\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.320470\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.496415\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.493303\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.467681\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.360684\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.323577\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.367125\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.402741\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.309486\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.400802\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.296849\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.471008\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.460509\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.456220\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.335524\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.308070\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.341471\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.388430\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.291287\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.387170\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.279365\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.452824\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.435322\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.449144\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.316476\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.297605\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.321247\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.377377\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.277446\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.376540\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.265832\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.439055\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.415325\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.444660\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.301324\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.289985\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.304425\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.369170\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.266659\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.367675\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.254951\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.428179\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.398911\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.441798\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "# define the twenty-network\n",
    "twenty_model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the twenty-network\n",
    "twenty_optimizer = optim.SGD(twenty_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "twenty_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the twenty-network\n",
    "    train(twenty_model, train_criterion, train_loader, twenty_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    twenty_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = twenty_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    twenty_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7548166666666667, 0.8387, 0.8612333333333333, 0.8741, 0.8824, 0.88795, 0.8927, 0.8961166666666667, 0.8991333333333333, 0.9019166666666667]\n"
     ]
    }
   ],
   "source": [
    "print(twenty_model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqezVf-ybfVR"
   },
   "source": [
    "Test the trained 20-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1JwFDxuubeki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3320, Accuracy: 9060/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(44)\n",
    "test(twenty_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYFzQ_AObdp0"
   },
   "source": [
    "Train the 50-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FpQiI7o-bzdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315129\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.249916\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.136567\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.907862\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.838665\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.745965\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.500738\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.687810\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.390489\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.229845\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.237610\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.237765\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.261933\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.905777\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.980741\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.069207\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.885865\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.265059\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.956839\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.800972\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.832054\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.942768\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.978472\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.645969\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.732880\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.877874\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.721003\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.101189\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.818943\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.672873\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.681912\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.822868\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.844440\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.547865\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.615417\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.784011\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.627127\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.866415\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.649696\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.590831\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.569714\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.526315\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.543655\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.493478\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.412679\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.522002\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.446043\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.601454\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.526767\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.513371\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.470332\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.419127\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.431438\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.443252\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.344092\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.447899\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.381732\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.538272\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.472929\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.478662\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.412775\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.368682\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.376509\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.414931\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.306543\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.412706\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.343365\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.504277\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.438953\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.460242\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.373576\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.339145\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.342321\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.395925\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.282920\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.392110\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.317511\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.481559\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.414639\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.449040\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.344947\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.320333\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.318355\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.381679\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.266615\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.378240\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.298235\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.464818\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.396224\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.441339\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.322521\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.307774\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.300000\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.370274\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.255074\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.368200\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.282964\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.451835\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.381380\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.435714\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "# define the fifty-network\n",
    "fifty_model = mnist_network(num_hidden_layers=1,layer_size=50,activation='relu')\n",
    "\n",
    "# create optimizer for the 50-network\n",
    "fifty_optimizer = optim.SGD(fifty_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "fifty_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the fifty-network\n",
    "    train(fifty_model, train_criterion, train_loader, fifty_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    fifty_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = fifty_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    fifty_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7414833333333334, 0.7884833333333333, 0.8048166666666666, 0.85305, 0.8762166666666666, 0.8851833333333333, 0.8915333333333333, 0.8960333333333333, 0.8997333333333334, 0.9027833333333334]\n"
     ]
    }
   ],
   "source": [
    "print(fifty_model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnTUT4Gkb0D3"
   },
   "source": [
    "Test the trained 50-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XcjuzPU-b7AU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3297, Accuracy: 9070/10000 (91%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "test(fifty_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JdznrIAb7jy"
   },
   "source": [
    "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lrzcDMwhcOJP"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lines\n",
    "plt.plot(range(1,epochs + 1), five_model_accuracies, label = \"5-network\")\n",
    "plt.plot(range(1,epochs + 1), twenty_model_accuracies, label = \"20-network\")\n",
    "plt.plot(range(1,epochs + 1), fifty_model_accuracies, label = \"50-network\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmC8m-SRcOkP"
   },
   "source": [
    "What is your conclustion on the effect of varying the hidden layer size on the performance of a neural network trained on the MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_Looking at the plot, we can tell its obvious that as the number of epochs increases, so does the accuracy. At the beginning, when we just start to train the model (epochs 1 - 3), the accuracy is low, especially for the 5-network. The best performing network right at 1 epoch is the 20-network with about a 76% accuracy. However, once we get to the end of the training period (completed the 10 epochs), the 20 and 50-network have around the same accuracy. Technically in the end, the 50-network is the best performing, but the 20-network is not that far away. After running this exact question multiple times and seeing different results each time, I conclude that by increasing the size of the hidden layer, the performance of the neural network increases, however once you get to a certain point, the size of the hidden layer doesnt improve much. This is evident especially in this example where the 20 and 50-network converge to the same accuracy. Since there is some variability in what you get when you run these networks multiple times, I noticed that the 20-network is the one that performs the most consistent. I believe this has to do with the tradeoff of being not too complex but not too generalized, which makes sense since it is the middle value between the two hidden layer sizes we are checking._ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdlMt7WdhE4b"
   },
   "source": [
    "## Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGBCmQHDa69o"
   },
   "source": [
    "Now, we will investigate the effect of varying the number of hidden layers. Create 3 networks with 1, 2, and 3 hidden layers, respectively. The size of all hidden layers should be 20 and the activation function is ReLU. We will call these the 1-network, the 2-network, and the 3-network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9RMn8b9cm9i"
   },
   "source": [
    "Train the 1-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "km6IknYlcm9j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300323\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.270432\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.201476\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.996546\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.075665\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.050701\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.841506\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.834021\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.638700\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.602660\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.639453\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.583329\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.502825\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.218866\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.526678\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.405886\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.152557\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.225761\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.125175\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.066173\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.115268\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.933764\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.941190\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.840117\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.012657\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.886770\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.794563\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.922297\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.899577\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.843634\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.895969\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.709302\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.754640\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.730685\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.873201\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.763594\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.667174\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.816228\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.793378\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.761845\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.801497\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.606245\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.670185\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.677388\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.799612\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.699800\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.601217\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.762136\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.726251\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.721545\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.743616\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.543945\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.617906\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.643435\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.750983\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.657615\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.559130\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.726689\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.682217\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.698144\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.704992\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.503106\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.581243\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.619145\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.716030\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.627489\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.530365\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.701424\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.650275\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.683165\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.675281\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.475077\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.554181\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.600664\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.689898\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.605081\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.509078\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.682496\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.626600\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.673314\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.652500\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.454641\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.532428\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.585636\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.669265\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.587989\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.492797\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.666984\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.608747\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.666421\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.633885\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.439820\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.513817\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.573967\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.653148\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.574414\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.479556\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.654324\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.593898\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.661157\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# define the one-network\n",
    "one_model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the 1-network\n",
    "one_optimizer = optim.SGD(one_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "one_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the one-network\n",
    "    train(one_model, train_criterion, train_loader, one_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    one_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = one_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    one_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5272166666666667, 0.7649666666666667, 0.7908166666666666, 0.8010333333333334, 0.8064666666666667, 0.8109166666666666, 0.81405, 0.8173333333333334, 0.8200166666666666, 0.82225]\n"
     ]
    }
   ],
   "source": [
    "print(one_model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHW3-J4vcm9k"
   },
   "source": [
    "Test the trained 1-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iROIK0lgcm9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5642, Accuracy: 8233/10000 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(68)\n",
    "test(one_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37h7g55ecm9n"
   },
   "source": [
    "Train the 2-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_c7TL_5Icm9o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 147.813065\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 107.258286\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 56.535053\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 35.952488\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 36.090221\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 43.245090\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 34.771687\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 36.024658\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 38.258026\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 21.605701\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 24.543703\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 21.082384\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 24.859507\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 26.597393\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 24.304577\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 38.142662\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 29.489254\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 30.918945\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 26.868320\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 18.053812\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 20.070467\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 20.166529\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 25.456087\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 23.954250\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 21.981848\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 37.390999\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 27.351784\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 27.144886\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 23.278694\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 17.508720\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 18.696701\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 20.093300\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 22.493500\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 23.661572\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 21.064383\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 34.687542\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 25.007320\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 24.737989\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 23.144726\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 16.835155\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 18.258291\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 19.753574\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 21.914869\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 23.707598\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 20.688402\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 31.613508\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 23.858313\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 23.128933\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 23.760801\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 16.747458\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 18.133429\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 19.166277\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 21.860373\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 23.268021\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 20.297272\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 28.720469\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 23.231812\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 22.528717\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 23.707573\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 16.458441\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 18.818737\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 19.420513\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 22.091330\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 21.630531\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 20.162928\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 26.446592\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 22.618652\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 21.408661\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 23.110392\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 15.537512\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 18.574884\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 18.926964\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 21.308584\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 21.852463\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 20.163048\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 24.645641\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 22.337013\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 22.240662\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 22.746609\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 15.480697\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 18.036083\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 18.609905\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 20.589458\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 22.058224\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 19.667774\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 22.638357\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 22.104620\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 21.345690\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 22.480400\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 14.646062\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 17.900415\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 18.856686\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 19.965069\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 21.728006\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 19.362116\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 22.035231\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 22.014202\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 20.351028\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 22.202906\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 14.607077\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# define the two-network\n",
    "two_model = mnist_network(num_hidden_layers=2,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the 2-network\n",
    "two_optimizer = optim.SGD(two_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "two_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the two-network\n",
    "    train(two_model, test_criterion, train_loader, two_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    two_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = two_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    two_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qbZfJ7Abbc2P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82575, 0.8458666666666667, 0.8558333333333333, 0.8611833333333333, 0.8644, 0.8665833333333334, 0.8674333333333333, 0.8688833333333333, 0.8698833333333333, 0.86875]\n"
     ]
    }
   ],
   "source": [
    "print(two_model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fJGIL1Hcm9p"
   },
   "source": [
    "Test the trained 2-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "m7Sr0On6cm9p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3641, Accuracy: 8629/10000 (86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(70)\n",
    "test(two_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3KteClMcm9q"
   },
   "source": [
    "Train the 3-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "thWHY7rWcm9r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301425\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.304820\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.310930\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.316746\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.309440\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.295924\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.273448\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.298952\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.298415\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.292529\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.276950\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.284822\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.293906\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.299333\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.285742\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.272441\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.244082\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.275678\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.276724\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.266499\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.245185\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.251587\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.263395\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.272935\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.249947\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.230774\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.188849\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.237790\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.238024\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.216606\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.187529\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.188361\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.205473\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.225239\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.186437\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.150706\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.081062\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.149340\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.149176\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.100663\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.062672\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.048918\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.068370\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.099226\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.042982\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.987222\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.861690\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.952474\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.951126\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.897619\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.809368\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 1.789343\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.876739\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.847153\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.769584\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.726602\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.589355\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.663719\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.650668\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.669628\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.475741\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.479069\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.722300\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 1.531123\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.504831\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.460600\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.388238\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 1.419261\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.387138\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.524394\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.228732\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.216633\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.601036\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.298849\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.297224\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.276620\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.256351\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 1.256576\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.195485\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 1.416153\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.054902\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 1.036175\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.482710\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.120837\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.124091\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.148896\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.152110\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 1.131982\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.058635\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.324798\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.931375\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.908521\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.354070\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.983326\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.979421\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.050746\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.061361\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 1.025218\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.956010\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.247487\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# define the three-network\n",
    "three_model = mnist_network(num_hidden_layers=3,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the 3-network\n",
    "three_optimizer = optim.SGD(three_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "three_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the three-network\n",
    "    train(three_model, train_criterion, train_loader, three_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    three_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = three_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    three_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10518333333333334, 0.26005, 0.31805, 0.35785, 0.3944666666666667, 0.4849833333333333, 0.6042, 0.6636666666666666, 0.7024166666666667, 0.7319]\n"
     ]
    }
   ],
   "source": [
    "print(three_model_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB4nTkYecm9s"
   },
   "source": [
    "Test the trained 3-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "176L8ou0cm9t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9307, Accuracy: 7328/10000 (73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(72)\n",
    "test(three_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CW0D0CWWcm9t"
   },
   "source": [
    "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "gW6JCxpmcm9t"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lines\n",
    "plt.plot(range(1,epochs + 1), one_model_accuracies, label = \"1-network\")\n",
    "plt.plot(range(1,epochs + 1), two_model_accuracies, label = \"2-network\")\n",
    "plt.plot(range(1,epochs + 1), three_model_accuracies, label = \"3-network\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65eqaTMbcm9u"
   },
   "source": [
    "What is your conclustion on the effect of varying the number of hidden layers on the performance of a neural network trained on the MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_Based on the plot above, the 2-network seems to perform  the best overall with an average accuracy of about 85%. Similar to the situation in part a, the accuracy still increases, but at a slower rate than before. The next highest performing network is the 1-network with an average accuracy of about 75% followed by the 3-network with an average accuracy of about 40%. Just like what I mentioned in the previous part, I believe that increasing the number of hidden layers might also increase complexity which in turn increases the variance. Conversely, decreasing the number of hidden layers might lead to a higher bias, via the bias variance trade off. This could be the reason why the 2-network, the middle/tradeoff level performs the best._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrIobY5hJe5"
   },
   "source": [
    "## Part c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX_rfiw5c6ZL"
   },
   "source": [
    "Next, we will investigate the effects of varying the activation functions on a neural network. Create 3 networks. The first network has Sigmoid activation (Sigmoid-network). The second network has ReLU activation (ReLU-network). The third network has Tanh activation (Tanh-network). All networks have one hidden layer with size 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py5w8S3Cd9E0"
   },
   "source": [
    "Train the Sigmoid-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "3dGgpGwyd9E2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.309737\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.314262\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.286859\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.310013\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.292585\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.295747\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.297938\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.298479\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.300939\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.285291\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.293699\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.298695\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 2.277181\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 2.291199\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.279037\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.281797\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.280839\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.287729\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.285729\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 2.270517\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.278337\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 2.283680\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.267487\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.273688\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.266148\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.268523\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 2.264659\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.277225\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 2.271497\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 2.256809\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.263953\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.269547\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 2.257998\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 2.258029\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.254230\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.256165\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 2.249894\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.267175\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.258590\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.244482\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.250751\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 2.256494\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.248896\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 2.244334\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 2.243371\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.244837\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 2.236676\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 2.257675\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.246977\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 2.233437\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.238728\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.244482\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 2.240206\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 2.232256\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.233404\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.234404\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.224769\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 2.248652\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.236380\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 2.223382\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.227684\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 2.233330\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.231881\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.221356\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 2.224126\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.224670\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.213873\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.240017\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.226540\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 2.214059\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.217412\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 2.222874\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.223874\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.211298\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.215388\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.215484\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 2.203758\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.231701\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.217286\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.205281\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.207760\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 2.212992\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.216144\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 2.201859\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 2.207082\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.206747\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.194259\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.223656\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 2.208503\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.196918\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.198616\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 2.203596\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.208652\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.192887\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.199125\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.198382\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 2.185260\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 2.215841\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.200111\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.188878\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# define the sigmoid-network\n",
    "sigmoid_model = mnist_network(num_hidden_layers=1,layer_size=20)\n",
    "sigmoid_model.activation = torch.nn.Sigmoid()\n",
    "\n",
    "# create optimizer for the sigmoid-network\n",
    "sigmoid_optimizer = optim.SGD(sigmoid_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "sigmoid_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the sigmoid-network\n",
    "    train(sigmoid_model, train_criterion, train_loader, sigmoid_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    sigmoid_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = sigmoid_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    sigmoid_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6PTSxcDd9E2"
   },
   "source": [
    "Test the trained Sigmoid-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "qKhk1nq7d9E3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.1926, Accuracy: 6371/10000 (64%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(72)\n",
    "test(sigmoid_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hpbsbN1d9E4"
   },
   "source": [
    "Train the ReLU-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "fu3wbDf2d9E7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 147.866287\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 51.347614\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 41.548210\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 27.462687\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 15.960865\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 20.419659\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 14.096617\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 23.897438\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 18.115345\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 20.187700\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 9.133462\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 9.445862\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 12.739426\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 15.625753\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 9.118756\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 13.663810\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 9.131331\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 18.964642\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 9.663550\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 15.764832\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 6.055115\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 7.088619\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 8.252902\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 14.682514\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 4.652898\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 12.403879\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 7.667763\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 17.899498\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 7.754225\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 13.128551\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 5.054745\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 5.853332\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 7.439354\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 13.164872\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 4.020091\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 9.802140\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 7.324432\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 18.195196\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 8.434607\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 11.581203\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 4.045313\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 5.390357\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 7.403188\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 10.242908\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 3.378960\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 8.073022\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 6.520534\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 17.405830\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 8.661250\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 11.330131\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 3.322226\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 4.369126\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 7.210120\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 8.786607\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 3.254717\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 6.801815\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 5.811037\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 17.351313\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 9.108102\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 10.787971\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.915003\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 3.724669\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 6.689352\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 8.097376\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 3.223291\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 5.815269\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 5.182032\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 16.981302\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 9.228239\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 10.623750\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.397822\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 3.583029\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 6.470306\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 8.199628\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 3.203232\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 5.462374\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 4.745663\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 16.788734\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 9.344823\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 10.521488\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.015013\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 3.341625\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 6.275197\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 8.530969\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 3.372402\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 5.259847\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 4.888470\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 15.785491\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 9.528292\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 10.151410\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.696137\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 3.398409\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 5.855876\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 8.980042\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 3.190843\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 5.069737\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 4.953362\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 15.235587\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 9.906035\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 9.902740\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# define the relu-network\n",
    "relu_model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the relu-network\n",
    "relu_optimizer = optim.SGD(relu_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "relu_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the relu-network\n",
    "    train(relu_model, test_criterion, train_loader, relu_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    relu_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = relu_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    relu_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYOANVPWd9E7"
   },
   "source": [
    "Test the trained ReLU-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Cw_lcZAsd9E8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1418, Accuracy: 9590/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(72)\n",
    "test(relu_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66X8ymnLd9E8"
   },
   "source": [
    "Train the Tanh-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "m8HulP31d9E9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 150.853760\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 81.846916\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 76.146347\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 72.989250\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 67.271271\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 69.263687\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 67.035652\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 68.369438\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 68.933815\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 64.686821\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 61.676388\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 62.779221\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 62.749035\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 63.402489\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 61.274143\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 64.712128\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 62.924973\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 64.448326\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 65.019363\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 62.074730\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 59.454937\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 60.512871\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 60.532177\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 60.947815\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 60.167400\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 63.532051\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 61.329956\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 62.905289\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 63.271671\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 60.780708\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 58.718796\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 59.387203\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 59.786156\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 59.837208\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 59.941650\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 62.650848\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 60.329651\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 61.539547\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 62.339874\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 59.993073\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 58.250111\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 58.640984\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 59.419357\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 59.076050\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 59.843441\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 62.044769\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 59.634178\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 60.760899\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 61.727932\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 59.410473\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 57.971092\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 58.289787\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 59.195354\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 58.534973\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 59.724800\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 61.914158\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 59.052532\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 60.347115\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 61.306751\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 58.772038\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 57.802345\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 58.082573\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 58.928463\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 58.156033\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 59.498016\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 62.086330\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 58.575150\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 60.117199\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 61.140919\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 58.175194\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 57.676708\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 57.968708\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 58.662209\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 57.895042\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 59.222641\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 62.325008\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 58.158066\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 59.899635\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 60.943672\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 57.735519\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 57.554779\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 57.876228\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 58.426437\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 57.695251\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 58.983204\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 62.459137\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 57.814167\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 59.635223\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 60.630608\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 57.429726\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 57.463127\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 57.767731\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 58.175274\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 57.483868\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 58.743946\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 62.396252\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 57.580372\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 59.357899\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 60.301113\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 57.177608\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# define the tanh-network\n",
    "tanh_model = mnist_network(num_hidden_layers=1,layer_size=20,activation='tanh')\n",
    "\n",
    "# create optimizer for the tanh-network\n",
    "tanh_optimizer = optim.SGD(tanh_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "tanh_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the tanh-network\n",
    "    train(tanh_model, test_criterion, train_loader, tanh_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    tanh_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = tanh_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    tanh_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYt6o7Aed9E-"
   },
   "source": [
    "Test the trained Tanh-network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "q7-YK5gVd9E_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9202, Accuracy: 9391/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(72)\n",
    "test(tanh_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIGx8kPTd9FA"
   },
   "source": [
    "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MVy4iVnMd9FA"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lines\n",
    "plt.plot(range(1,epochs + 1), sigmoid_model_accuracies, label = \"sigmoid-network\")\n",
    "plt.plot(range(1,epochs + 1), relu_model_accuracies, label = \"relu-network\")\n",
    "plt.plot(range(1,epochs + 1), tanh_model_accuracies, label = \"tanh-network\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCGjP911d9FB"
   },
   "source": [
    "What is your conclusion on the effect of varying the activation functions on the performance of a neural network trained on MNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_By varying the activation functions, we see that the best performing activation function is relu. Closely following relu is tanh and the worst performing activation function is sigmoid. Although there isn't much of a difference if we change the activation function from relu to tanh and vice versa (since they both end at a value of about 96% /97%), we see that the sigmoid, as expected, follows a logistic curve and only converges to 63% at the end of the 10th epoch. So using tanh or relu would be the best suggestion compared to sigmoid for the activation function._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiAZ36IdhMvG"
   },
   "source": [
    "## Part d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8H1N3i-derH7"
   },
   "source": [
    "Finally, we will look into the effect of varying the value of the learning rate on the performance of a neural network. Create a network with one hidden layer of size 20 and ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUFgUcGUfG7z"
   },
   "source": [
    "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to be 0.1. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "KtLuEUNKfG72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298168\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.539162\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.419262\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.528268\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.363742\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.648163\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.468586\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.537555\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.457820\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.569704\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.273476\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.392620\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.251340\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.457069\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.298622\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.588504\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.391574\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.479404\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.463274\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.495992\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.234461\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.378916\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.212603\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.427529\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.273641\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.522061\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.387018\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.450986\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.478079\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.506889\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.229559\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.381595\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.206000\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.404595\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.263542\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.439871\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.386897\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.423351\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.502801\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.510058\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.225191\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.366114\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.208777\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.400515\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.267852\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.392475\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.378864\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.405813\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.499285\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.505487\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.213727\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.371756\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.198216\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.394514\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.251939\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.369869\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.378156\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.401423\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.484841\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.502704\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.208015\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.361138\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.199653\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.399856\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.246657\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.353514\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.373985\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.398301\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.469386\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.501259\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.202048\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.348364\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.202384\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.392438\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.244433\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.335555\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.366372\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.391547\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.473699\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.500044\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.200790\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.333394\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.208103\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.389446\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.239224\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.326979\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.362582\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.385196\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.490812\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.507537\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.195544\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.326008\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.201037\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.392607\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.230887\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.322492\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.345910\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.375247\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.479754\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.506618\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# define the tenth-network\n",
    "tenth_model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the tenth-network\n",
    "tenth_optimizer = optim.SGD(tenth_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "tenth_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the tenth-network\n",
    "    train(tenth_model, train_criterion, train_loader, tenth_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    tenth_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = tenth_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    tenth_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC5CD3zmfG73"
   },
   "source": [
    "Test the trained network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "yT04G-BLfG74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3657, Accuracy: 8667/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(72)\n",
    "test(tenth_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REVtrsiqfG75"
   },
   "source": [
    "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to be 0.01. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "aEXtS_XEfG76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.327361\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.246654\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.070409\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.580138\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.373426\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.437107\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.331501\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.472921\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.431582\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.414553\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.298457\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.314229\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.238759\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.372703\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.231804\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.345702\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.225372\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.388469\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.347101\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.400886\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.219244\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.271283\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.179968\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.340218\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.197708\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.323974\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.194837\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.356118\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.297674\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.375540\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.179130\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.249406\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.151430\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.319903\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.178024\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.299186\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.178698\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.336441\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.258107\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.346958\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.151958\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.225585\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.134564\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.304347\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.162753\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.277836\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.165409\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.321603\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.234749\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.325140\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.135020\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.213963\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.124338\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.294943\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.150169\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.260690\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.154807\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.311053\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.214258\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.310092\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.123616\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.210907\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.115835\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.287507\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.140906\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.248015\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.146066\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.297317\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.194931\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.297995\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.116570\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.209748\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.110278\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.277132\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.132414\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.237824\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.138984\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.289658\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.182690\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.287579\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.108686\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.208484\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.104481\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.259883\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.124707\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.228642\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.133002\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.283597\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.171164\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.280859\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.101355\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.209102\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.099164\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.243907\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.119266\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.218609\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.126542\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.277262\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.161739\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.276072\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# define the hund-network\n",
    "hund_model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the hund-network\n",
    "hund_optimizer = optim.SGD(hund_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "hund_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the hund-network\n",
    "    train(hund_model, train_criterion, train_loader, hund_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    hund_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = hund_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    hund_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2dLcxgdfG77"
   },
   "source": [
    "Test the trained network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "pT9_RHk_fG78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1871, Accuracy: 9419/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(72)\n",
    "test(hund_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJu3jwdKfG79"
   },
   "source": [
    "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to 0.001. After each epoch, record the current training accuracy of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "U_GDwCxkfG7-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.333499\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.258330\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.213496\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.137605\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.142013\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.114271\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.062809\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.001007\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.875035\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.715694\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.908723\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.660475\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.599771\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.559290\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.573894\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 1.516460\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.574278\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.622074\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.515048\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 1.319931\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.569622\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.243169\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.220977\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.145627\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.249768\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.166278\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.134140\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 1.250036\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.163188\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.973863\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.190843\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.939182\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.031045\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.967627\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.107133\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.025551\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.968332\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 1.111231\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.042991\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.873941\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.071918\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.823305\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.922424\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.889896\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.025774\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.950939\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.881668\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.035075\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.972786\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.818708\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.998341\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.755497\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.850055\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.842684\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.966493\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.903832\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.810217\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.931184\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.773469\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.737393\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.802130\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.601604\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.682836\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.750133\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.803483\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.726649\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.579999\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.790301\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.699509\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.676058\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.722894\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.542617\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.610860\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.693950\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.748492\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.664192\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.528876\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.737779\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.667501\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.648227\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.681951\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.509289\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.567396\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.658840\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.712061\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.625020\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.498343\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.706610\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.644785\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.631970\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.653051\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.485986\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.536539\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.634958\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.686850\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.598203\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.476895\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.686325\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.628639\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.620292\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(6)\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# define the thou-network\n",
    "thou_model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
    "\n",
    "# create optimizer for the thou-network\n",
    "thou_optimizer = optim.SGD(thou_model.parameters(),lr=lr)\n",
    "\n",
    "# initialize an emp\n",
    "thou_model_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    # Training the thou-network\n",
    "    train(thou_model, train_criterion, train_loader, thou_optimizer, epoch)\n",
    "        \n",
    "    # use test function to get the performance \n",
    "    thou_model.eval()\n",
    "    train_loss = 0\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            output = thou_model(data)\n",
    "            train_loss += test_criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    thou_model_accuracies.append(num_correct / len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HbaGOWRfG7_"
   },
   "source": [
    "Test the trained network on the test data. Print out the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "hp6jgPocfG8A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.5856, Accuracy: 8210/10000 (82%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(72)\n",
    "test(thou_model, test_criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCAv1ujIfG8A"
   },
   "source": [
    "Plot the training accuracies over the epochs of the scenarios on the same figure (there should 3 line plots/scatter plots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "iTazEXPkfG8B"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot lines\n",
    "plt.plot(range(1,epochs + 1), tenth_model_accuracies, label = \"0.1-network\")\n",
    "plt.plot(range(1,epochs + 1), hund_model_accuracies, label = \"0.01-network\")\n",
    "plt.plot(range(1,epochs + 1), thou_model_accuracies, label = \"0.001-network\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLal-1hxfG8B"
   },
   "source": [
    "What is your conclustion on the effect of varying the learning rate on the performance of a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_Based on the plot above, the middle higher learning rate produces higher/better performing models. The best, 0.01-network, ends with an accuracy score of about 97%. Following that is the 0.1-network with about a 88% and finally the 0.001-network with about an 82%. This also follows my theory on the bias variance tradeoff since the best performing network (0.01) is the middle / tradeoff between the two. I find it interesting that although the higher learning rates have overall higher performance values, the 0.001-network increases the fastest over all the epochs._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9FJtJgufv-n"
   },
   "source": [
    "## REMARK for Problem 2\n",
    "\n",
    "You have observed the effects of varying different hyperparameters on the performance of a neural network **on the MNIST dataset**. However, keep in mind that these trends only apply for **the MNIST dataset** and should not be carried to another problem. There is no single hyperparameter settings that works for all problems. As you do more problems, you will build up your intuitions about the hyperparameters so that you can quickly deploy a good model. For example, people observed that setting the learning rate = 0.001 often works the best, though it is not always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHSaycfVdHRJ"
   },
   "source": [
    "## Problem 3\n",
    "\n",
    "Experimenting with **k-anomity, i-diversity, and t-closeness**. \n",
    "\n",
    "Consider a dataset, for example, with 3 ordinary attributes and 1 sensitive attribute. Let the 3 ordinary attributes be Age, Sex, and Education and the sensitive attribute be Income, each row in this dataset is of the form:\n",
    "\n",
    "$$\n",
    "    [Age, Sex, Education, Income]\n",
    "$$\n",
    "\n",
    "A hacker is interested in knowing the sensitive attribute Income. When the dataset is designed so that if complies with either **k-anomity**, **i-diversity**, and/or **t-closeness**, even if he or she somehow figures out the values of the three, the hacker may not retrive the sensitive information accurately. In general, **k-anomity** is weaker than **i-diversity**, which, in turn, is weaker than **t-closeness**.\n",
    "\n",
    "By definition, **k-anomity** means that there is at least **k** different rows in the table of which ordinary values are a particular combination of Age, Sex, and Education. For example, the hacker knows the information of the person of interest is Age = 31, Sex = Female, and Education = BS. He or she looks into the data table and found that there are 3 rows with that combination:\n",
    "\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=300k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=70k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "\n",
    "The hacker cannot tell accurately what the income of the person is because it can be one of the 3 values shown. This particular combination of information has 3-anomity. If every combination corresponds to at least 3 rows, then the dataset has 3-anomity.\n",
    "\n",
    "a) Let's look at the dataset **\"table.csv\"**. Let the sensitive attribute be **education** and others be ordinary attributes. Calculate the anomity of the dataset (the value **k**). First, find all the posible combinations of the ordinary attributes that exists in the dataset. After that, determine the anomity for each combination. The anomity of the dataset is the smallest anomity among the combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "Qrm7S5y5dHRJ",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>11th</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>32556</td>\n",
       "      <td>27</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>32557</td>\n",
       "      <td>40</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>32558</td>\n",
       "      <td>58</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>32559</td>\n",
       "      <td>22</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>32560</td>\n",
       "      <td>52</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  age   education   race     sex\n",
       "0               0   39   Bachelors  White    Male\n",
       "1               1   50   Bachelors  White    Male\n",
       "2               2   38     HS-grad  White    Male\n",
       "3               3   53        11th  Black    Male\n",
       "4               4   28   Bachelors  Black  Female\n",
       "...           ...  ...         ...    ...     ...\n",
       "32556       32556   27  Assoc-acdm  White  Female\n",
       "32557       32557   40     HS-grad  White    Male\n",
       "32558       32558   58     HS-grad  White  Female\n",
       "32559       32559   22     HS-grad  White    Male\n",
       "32560       32560   52     HS-grad  White  Female\n",
       "\n",
       "[32561 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.read_csv('table.csv')\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Female</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Female</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Male</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Male</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <th>Other</th>\n",
       "      <th>Female</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unnamed: 0  education\n",
       "age race               sex                          \n",
       "49  Amer-Indian-Eskimo Female           1          1\n",
       "48  Amer-Indian-Eskimo Female           1          1\n",
       "74  Amer-Indian-Eskimo Male             1          1\n",
       "48  Amer-Indian-Eskimo Male             1          1\n",
       "74  Other              Female           1          1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.groupby(['age','race','sex']).count().sort_values('Unnamed: 0').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMENT\n",
    "_To calculate the k-anonymity, I took the dataset, grouped together the ordinary attributes and counted all the unique combinations of those unique combinations. Based on the code above where I find the counts of each unique combination, the lowest number of observations with a specific combination is 1. Thus since we can easily figure out the identity of those individuals with a combination that only refers to them within this dataset, our k-anonymity is 1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixGLowBjdHRK"
   },
   "source": [
    "We can improve the **k-anomity** of the dataset by \"suppressing\" the ordinary attributes. Suppressing means reducing the resolution of the attribute's value. For this problem, let's suppress Age by replacing the exact age with an age range. For example, instead of leaving age = 32, replace it with age = 30-40. Apply this to **\"table.csv\"** with the ranges {<20, 20-30, 30-50, >50}. Check if the anomity improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "AGqI5mltdHRK",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30-50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30-50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>30-50</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&gt;50</td>\n",
       "      <td>11th</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20-30</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>30-50</td>\n",
       "      <td>Masters</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>30-50</td>\n",
       "      <td>9th</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>&gt;50</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>30-50</td>\n",
       "      <td>Masters</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>30-50</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    age  education   race     sex\n",
       "0           0  30-50  Bachelors  White    Male\n",
       "1           1  30-50  Bachelors  White    Male\n",
       "2           2  30-50    HS-grad  White    Male\n",
       "3           3    >50       11th  Black    Male\n",
       "4           4  20-30  Bachelors  Black  Female\n",
       "5           5  30-50    Masters  White  Female\n",
       "6           6  30-50        9th  Black  Female\n",
       "7           7    >50    HS-grad  White    Male\n",
       "8           8  30-50    Masters  White  Female\n",
       "9           9  30-50  Bachelors  White    Male"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a simple function that can be applied to the table\n",
    "def age_range(age):\n",
    "    if age < 20:\n",
    "        return '<20'\n",
    "    elif age <= 30:\n",
    "        return '20-30'\n",
    "    elif age <= 50:\n",
    "        return '30-50'\n",
    "    else:\n",
    "        return '>50'\n",
    "\n",
    "table['age'] = table['age'].apply(age_range)  \n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">&lt;20</th>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Male</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <th>Male</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&gt;50</th>\n",
       "      <th>Other</th>\n",
       "      <th>Female</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">&lt;20</th>\n",
       "      <th>Asian-Pac-Islander</th>\n",
       "      <th>Male</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amer-Indian-Eskimo</th>\n",
       "      <th>Female</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Unnamed: 0  education\n",
       "age race               sex                          \n",
       "<20 Amer-Indian-Eskimo Male             4          4\n",
       "    Other              Male             5          5\n",
       ">50 Other              Female           7          7\n",
       "<20 Asian-Pac-Islander Male             8          8\n",
       "    Amer-Indian-Eskimo Female           9          9"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running the same code from earlier to check value counts\n",
    "table.groupby(['age','race','sex']).count().sort_values('Unnamed: 0').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_After supressing the ordinary attributes, we were able to find that at least 4 indivduals make up every single unique combination of ordianry attributes in this dataset. Hence the k-anonymity is 4._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv-RmXqCdHRK"
   },
   "source": [
    "**K-anomity** is nice, however, it fails in many cases. If the rows which share a combination of ordinary attributes have only a few values for the sensitive attribute, then it is not much better than having no anomity at all. For example, consider:\n",
    "\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=300k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "$$\n",
    "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
    "$$\n",
    "\n",
    "When **k-anomity** fails in the second case, **i-diversity** comes to the rescue. **I-diversity** states that the rows of a particular combination of information must have at least i different values for the sensitive attribute. The above example has 2-diversity, which is not good. \n",
    "\n",
    "b) Calculate the **i-diversity** of the dataset **\"table.csv\"**. Follow similar steps as in part a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "eZYCvcukdHRK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age  race                sex   \n",
       "41   Amer-Indian-Eskimo  Female     1\n",
       "67   Asian-Pac-Islander  Male       1\n",
       "     Amer-Indian-Eskimo  Male       1\n",
       "39   Amer-Indian-Eskimo  Male       1\n",
       "66   Other               Female     1\n",
       "                                   ..\n",
       "40   White               Male      16\n",
       "64   White               Male      16\n",
       "27   White               Male      16\n",
       "52   White               Male      16\n",
       "32   White               Male      16\n",
       "Name: education, Length: 546, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset\n",
    "table = pd.read_csv('table.csv')\n",
    "\n",
    "# the following code shows the number of unique values for education for every combination\n",
    "table.groupby(['age','race','sex'])['education'].nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_To calculate the i-diversity, I grouped together the ordinary attributes, but instead of just counting the number of observations for each combination, I counted the number of each unique Education response for each combination. Based on the output above, the lowest number of unique values for education for each posssible combination is 1 when there is no generalization. Therefore the i-diversity is 1._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qWt9JvTdHRK"
   },
   "source": [
    "Suppressing an attribute can also improve the **i-diversity** of the dataset. Repeat the suppression as in **part a** and check if the diversity improves. If it does not, consider further suppress age by using the range {<20, 20-50, >50}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "FFVPKWrodHRK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    race                sex   \n",
       "<20    Amer-Indian-Eskimo  Male       3\n",
       "                           Female     4\n",
       ">50    Other               Female     4\n",
       "<20    Asian-Pac-Islander  Male       4\n",
       "                           Female     5\n",
       "       Other               Male       5\n",
       "                           Female     5\n",
       ">50    Amer-Indian-Eskimo  Female     6\n",
       "<20    Black               Male       6\n",
       "20-30  Amer-Indian-Eskimo  Female     7\n",
       "<20    Black               Female     8\n",
       "20-30  Amer-Indian-Eskimo  Male       9\n",
       ">50    Other               Male      10\n",
       "<20    White               Male      10\n",
       ">50    Amer-Indian-Eskimo  Male      10\n",
       "<20    White               Female    11\n",
       "30-50  Amer-Indian-Eskimo  Female    11\n",
       "       Other               Female    12\n",
       "20-30  Other               Female    12\n",
       "       Black               Female    12\n",
       "       Asian-Pac-Islander  Male      12\n",
       ">50    Asian-Pac-Islander  Female    12\n",
       "30-50  Amer-Indian-Eskimo  Male      13\n",
       "20-30  Other               Male      13\n",
       "       Black               Male      13\n",
       "       Asian-Pac-Islander  Female    14\n",
       ">50    Black               Female    15\n",
       "       Asian-Pac-Islander  Male      15\n",
       "30-50  Black               Male      15\n",
       "       Asian-Pac-Islander  Male      15\n",
       "                           Female    15\n",
       "       Other               Male      15\n",
       ">50    Black               Male      16\n",
       "30-50  White               Male      16\n",
       "                           Female    16\n",
       "       Black               Female    16\n",
       "20-30  White               Male      16\n",
       "                           Female    16\n",
       ">50    White               Female    16\n",
       "                           Male      16\n",
       "Name: education, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table['age'] = table['age'].apply(age_range)  \n",
    "table.groupby(['age','race','sex'])['education'].nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_After suppressing the dataset, so the age is a range, we see that there are at least 3 different education responses for every unique combination of ordinary attributes. Thus the updated i-diversity is 3._\n",
    "\n",
    "**Gonna try now by fixing the bounds a little more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age    race                sex   \n",
       "<20    Amer-Indian-Eskimo  Male       3\n",
       ">50    Other               Female     4\n",
       "<20    Amer-Indian-Eskimo  Female     4\n",
       "       Asian-Pac-Islander  Male       4\n",
       "       Other               Male       5\n",
       "                           Female     5\n",
       "       Asian-Pac-Islander  Female     5\n",
       ">50    Amer-Indian-Eskimo  Female     6\n",
       "<20    Black               Male       6\n",
       "                           Female     8\n",
       ">50    Other               Male      10\n",
       "       Amer-Indian-Eskimo  Male      10\n",
       "<20    White               Male      10\n",
       "                           Female    11\n",
       "20-50  Amer-Indian-Eskimo  Female    11\n",
       ">50    Asian-Pac-Islander  Female    12\n",
       "20-50  Other               Female    13\n",
       "       Amer-Indian-Eskimo  Male      13\n",
       "       Black               Male      15\n",
       "       Asian-Pac-Islander  Male      15\n",
       ">50    Asian-Pac-Islander  Male      15\n",
       "       Black               Female    15\n",
       "20-50  Asian-Pac-Islander  Female    15\n",
       "       White               Male      16\n",
       "                           Female    16\n",
       "       Other               Male      16\n",
       "       Black               Female    16\n",
       ">50    Black               Male      16\n",
       "       White               Female    16\n",
       "                           Male      16\n",
       "Name: education, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset\n",
    "table = pd.read_csv('table.csv')\n",
    "\n",
    "# define a simple function that can be applied to the table\n",
    "def new_age_range(age):\n",
    "    if age < 20:\n",
    "        return '<20'\n",
    "    elif age <= 50:\n",
    "        return '20-50'\n",
    "    else:\n",
    "        return '>50'\n",
    "    \n",
    "# apply the new function to the table again and figure out i-diversity\n",
    "table['age'] = table['age'].apply(new_age_range)  \n",
    "table.groupby(['age','race','sex'])['education'].nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_Even after suppressing the age column more, we still have at least 3 unique education responses per each unique combination so the i-diversity is still 3._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX1lsm8ydHRK"
   },
   "source": [
    "**T-closeness** is even better than **i-diversity**. **T-closeness** requires that for every combination of information, the distribution of the sensitive attribute's value among the corresponding rows must be close to the overall distribution of the sensitive attribute's value for the whole dataset. Distance between distribution is calculated using the Earth Mover Distance (EMD). The dataset has **t-closeness** if no distance exceeds **t**. \n",
    "\n",
    "c) Calculate the overall distribution of **education**. Find the **t-closeness** of the dataset (largest distance between any combination's distribution of marital-status and the overall distribution).\n",
    "\n",
    "You can use **scipy.stats.wasserstein_distance** to calculate the EMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "BjR4iG2bdHRK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "wasserstein_distance([0, 1, 3], [5, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "- length of the two lists doesnt need to be equal\n",
    "- loop through all the different categories and get the values\n",
    "- use that in the wasserstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32556</th>\n",
       "      <td>32556</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32557</th>\n",
       "      <td>32557</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32558</th>\n",
       "      <td>32558</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32559</th>\n",
       "      <td>32559</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>32560</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32561 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  age  education   race     sex\n",
       "0               0   39          0  White    Male\n",
       "1               1   50          0  White    Male\n",
       "2               2   38          1  White    Male\n",
       "3               3   53          2  Black    Male\n",
       "4               4   28          0  Black  Female\n",
       "...           ...  ...        ...    ...     ...\n",
       "32556       32556   27          6  White  Female\n",
       "32557       32557   40          1  White    Male\n",
       "32558       32558   58          1  White  Female\n",
       "32559       32559   22          1  White    Male\n",
       "32560       32560   52          1  White  Female\n",
       "\n",
       "[32561 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload the dataset\n",
    "table = pd.read_csv('table.csv')\n",
    "\n",
    "# facotrize such that\n",
    "#table.iloc[:,1] = pd.factorize(table.iloc[:,1])[0]\n",
    "#table.iloc[:,3] = pd.factorize(table.iloc[:,3])[0]\n",
    "table.iloc[:,2] = pd.factorize(table.iloc[:,2])[0]\n",
    "#table.iloc[:,4] = pd.factorize(table.iloc[:,4])[0]\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ages = np.sort(table.iloc[:,1].unique())\n",
    "unique_races = np.sort(table.iloc[:,3].unique())\n",
    "unique_sex = np.sort(table.iloc[:,4].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new variable to hold the actual distribution of education_values\n",
    "edu_vals = table.iloc[:,2]\n",
    "\n",
    "# store the wass_distances in a list\n",
    "wass_dists = []\n",
    "\n",
    "# iterate through every possible combination to find the education values\n",
    "for age in unique_ages:\n",
    "    for race in unique_races:\n",
    "        for sex in unique_sex:\n",
    "            \n",
    "            # store new table in its own variable to get the education values\n",
    "            use_df = table[(table['age'] == age) & (table['race'] == race) & (table['sex'] == sex)]\n",
    "            \n",
    "            # in the case where the combination does exist, input the subset of edu values\n",
    "            if len(use_df) != 0:\n",
    "                wass_dists.append(wasserstein_distance(use_df.iloc[:,2], edu_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.575535149411873"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the largest distance which is the t in t-closeness\n",
    "max(wass_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comment\n",
    "_To find the t-closeness, we have to compare the distribution of education values for each unique combination of ordinary attributes with the distribution of educations values for the whole dataset. The t in t- closeness represent the maximum distance in the whole dataset. Using wasserstein_distance, we can quantify that distance. After iterating through every unique combination, I get a max distance, or t of about 11.57 / 12._"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "p9FJtJgufv-n"
   ],
   "name": "HW3_F21.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
