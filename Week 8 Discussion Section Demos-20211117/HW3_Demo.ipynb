{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "p9FJtJgufv-n"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNnrjkIUWEDh"
      },
      "source": [
        "# Homework 3\n",
        "\n",
        "DUE Nov 15th at 11:59 PM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbpTqmT4WhaB"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "In this problem, you will implement a simple feed-forward neural network using PyTorch, a straight-forward and simple-to-pickup framework for quickly prototyping deep learning model. \n",
        "\n",
        "PyTorch provides 2 powerful things. First, a nice data structure called Tensor (basically a matrix, similar to Numpy ndarray). Tensor is optimized for matrix calculation and can be loaded to a GPU. Tensor is also implemented so that it's easy to calculate and pass back chains of gradients, which is extremely useful for backpropagation on neural network. Second, a nice inner mechanism called Autograd that nicely maps variables involved a chain of calculations and efficiently calculates their gradients via the chain rule when needed. Read more here: https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95  \n",
        "\n",
        "You will define a neural network class in PyTorch and use the network to learn a classification task on the famous KDD CUP 99 dataset. You can refer to Problem 2 to see how a network class can be defined, how to use a PyTorch's DataLoader, and how a training loop may looks like.\n",
        "\n",
        "There are many greate tutorial on PyTorch out there. For example, this video on Youtube explains how to build a simple network in PyTorch quite clearly: https://www.youtube.com/watch?v=oPhxf2fXHkQ\n",
        "\n",
        "### Part a\n",
        "Firstly, load and inspect the \"**KDD CUP 99**\" dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyL0okEBWggC"
      },
      "source": [
        "from sklearn.datasets import fetch_kddcup99\n",
        "\n",
        "X, y = fetch_kddcup99(return_X_y=True, percent10=True)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD09sAm-52_u"
      },
      "source": [
        "Split them into a train set (70%), a validation set (20%), and a test set (20%). Then, create a PyTorch's DataLoader for the train set, a DataLoader for the validation set, and a DataLoader for the test set.\n",
        "\n",
        "You can read about PyTorch's DataLoader from:\n",
        "\n",
        "*   https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
        "*   https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnwd2ylj5BD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb5735b-e7e6-4f1f-ba9c-c00b63894f33"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Your answer goes here\n",
        "\n",
        "print(X)\n",
        "\n",
        "# As you can see, we have some categorical values for a few of our rows\n",
        "\n",
        "print(X[:, 1])\n",
        "\n",
        "# We can fix this by going back to the Data Cleaning demo I showed in week 1\n",
        "# Essentially, we can use a label encoder to change the categorical values into numerical ones"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 b'tcp' b'http' ... 0.0 0.0 0.0]\n",
            " [0 b'tcp' b'http' ... 0.0 0.0 0.0]\n",
            " [0 b'tcp' b'http' ... 0.0 0.0 0.0]\n",
            " ...\n",
            " [0 b'tcp' b'http' ... 0.01 0.0 0.0]\n",
            " [0 b'tcp' b'http' ... 0.01 0.0 0.0]\n",
            " [0 b'tcp' b'http' ... 0.01 0.0 0.0]]\n",
            "[b'tcp' b'tcp' b'tcp' ... b'tcp' b'tcp' b'tcp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88M9h4dqXpTQ",
        "outputId": "07a65a7a-4b2f-4d1d-e553-3f87cbbb8a04"
      },
      "source": [
        "# Reminder of how to use a label encoder\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "myString = \"hereishowyoucanusealabelencoder\"\n",
        "dummyArray = []\n",
        "for char in myString:\n",
        "  dummyArray.append(char)\n",
        "print(dummyArray)\n",
        "\n",
        "chars = preprocessing.LabelEncoder()\n",
        "dummyArray = chars.fit_transform(dummyArray)\n",
        "print(dummyArray)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['h', 'e', 'r', 'e', 'i', 's', 'h', 'o', 'w', 'y', 'o', 'u', 'c', 'a', 'n', 'u', 's', 'e', 'a', 'l', 'a', 'b', 'e', 'l', 'e', 'n', 'c', 'o', 'd', 'e', 'r']\n",
            "[ 5  4 10  4  6 11  5  9 13 14  9 12  2  0  8 12 11  4  0  7  0  1  4  7\n",
            "  4  8  2  9  3  4 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLWrOnRUaNn6"
      },
      "source": [
        "To instantiate a dataloader into our model, let's take a look at the website that was provided that walks through the dataloaders\n",
        "\n",
        "Yingrui also gave a good implementation of it in her discussion section last week. This implementation can be seen below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYCrRzVjaIZR"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, X, y):\n",
        "        'Initialization'\n",
        "        self.labels = torch.tensor(y)\n",
        "        self.features = torch.tensor(X)\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "\n",
        "        return self.features[index], self.labels[index]\n",
        "\n",
        "# After we train, validate, split our data, we can just create Datasets\n",
        "# from our dataset class and use those to create Dataloaders\n",
        "\n",
        "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "# If we look at the Dataloader class, we can look at how to create them and what\n",
        "# parameters we need"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qgXybc1Zcvy"
      },
      "source": [
        "### Example Code from Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaAqMJuCZup3"
      },
      "source": [
        "To solve problems 1 and 2, we can look at the example code from question 2 as a guideline for what we are doing. To be able to adapt our code for problem 2, let's figure out how each of these things are working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Fy4G8NZcIH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Here we are creating a class for the MNIST dataset\n",
        "class mnist_network(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, num_hidden_layers=1, layer_size = 100, activation=None):\n",
        "        super(mnist_network, self).__init__()\n",
        "        # In the initialization function, we are defining the parameters given\n",
        "        # to us from our function call and storing these values within our model\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.layer_size = layer_size\n",
        "        self.activation = activation\n",
        "\n",
        "        if(self.activation is 'relu'):\n",
        "            self.activation = F.relu\n",
        "        elif(self.activation is 'tanh'):\n",
        "            self.activation = torch.tanh\n",
        "\n",
        "        # This line of code looks at the first layer in our model, which is the\n",
        "        # input model. The size of the input for our neural network is the number\n",
        "        # of features our data has\n",
        "        # 784 is specific to the MNIST dataset, and will not work for KDD CUP 99\n",
        "        self.layers = nn.ModuleList([nn.Linear(784,self.layer_size)])\n",
        "\n",
        "        # This next loop is only necessary if you have more than 1 hidden layer\n",
        "        # but works regardless of how many hidden layers you have\n",
        "        # What this does is creates a loop and maps each layer size to another\n",
        "        # hidden layer of that size\n",
        "        for i in range(1, self.num_hidden_layers):\n",
        "            self.layers.append(nn.Linear(self.layer_size,self.layer_size))\n",
        "\n",
        "        # And this last layer is the output layer, which is the number of classes\n",
        "        # we have for our dataset. Again, 10 is specific to the number of output\n",
        "        # classes for MNIST, you will have to change this for problem 1\n",
        "        self.layers.append(nn.Linear(self.layer_size,10))\n",
        "\n",
        "        # As we see, the initialization function created all our layers and \n",
        "        # made them the correct size\n",
        "\n",
        "    # Now we can define a forward function for when we are moving forward through\n",
        "    # the layers of our network\n",
        "    def forward(self, x):\n",
        "\n",
        "        # converting each image into a vector\n",
        "        # Your input vectors will be the wrong size without these two lines\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape(batch_size,-1)\n",
        "\n",
        "        # Now we loop through all our layers and use the activation method we \n",
        "        # input into our model\n",
        "        for i in range(self.num_hidden_layers+1):\n",
        "            x = self.layers[i](x)\n",
        "            if(self.activation is not None):\n",
        "                x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn_0S1s1Zi_7"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "\n",
        "# DataLoader is a nice tool provided by PyTorch for passing training or testing examples\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=64)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJJ3HNfPZtV9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "\n",
        "# Here we are creating two methods to train and test our model \n",
        "\n",
        "def train(model, criterion, train_loader, optimizer, epoch):\n",
        "    # Turn the model to training mode (gradients will be calculated)\n",
        "    model.train()\n",
        "    # Loop through our dataloader\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # We have to call zero_grad() on the optimizer to remove gradients from the previous data pass.\n",
        "        # Otherwise, the gradients will be accumulated throughout many passes.\n",
        "        optimizer.zero_grad()\n",
        "        # Pass in the data and obtain the output.\n",
        "        # When you pass the data directly by calling model(data), the model will internally pass the data through the forward() function.\n",
        "        output = model(data)\n",
        "        # Compare the output and the ground truth and calculate the loss.\n",
        "        loss = criterion(output, target)\n",
        "        # From the calculated loss, call backward() to calculate the gradients for all the paramters in the network.\n",
        "        loss.backward()\n",
        "        # Update the parameters according to the gradients. \n",
        "        optimizer.step()\n",
        "        \n",
        "        # This is to print out the progress you see when you run your code\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, criterion, test_loader):\n",
        "    # Turn the model to testing mode (gradients will not be calculated)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        # Loop through our data in the dataloader\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            # sum up batch loss\n",
        "            test_loss += criterion(output, target).item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True) \n",
        "            # sum up the correct number of items that you have guessed\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgOzJVRog2No"
      },
      "source": [
        "epochs = 5\n",
        "lr = 0.01\n",
        "\n",
        "model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
        "\n",
        "# Define the training and testing loss as cross entropy\n",
        "train_criterion = nn.CrossEntropyLoss()\n",
        "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# Define the optimizer\n",
        "# We have to specify the learning rate and the parameters that the optimizer should update during training.\n",
        "# In this case, we specify that the optimizer should update all the parameters from our model.\n",
        "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
        "\n",
        "# Now for every epoch, it will go through and train and test our model\n",
        "for epoch in range(1, epochs + 1):\n",
        "        # Training\n",
        "        train(model, train_criterion, train_loader, optimizer, epoch)\n",
        "        # Testing\n",
        "        test(model, test_criterion, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ37AcBjhMDB"
      },
      "source": [
        "Once you define your model classes and dataloaders, these last 2 cells should be relatively the same, as training, testing, and looping through the epochs should not change much from model to model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuEYUkVdWpYw"
      },
      "source": [
        "### Part b \n",
        "Create a Python class for our neural network model. The network should have 1 input layer, 1 hidden layer, and 1 output layer. You are free to choose the size of the hidden layer (it may affect the performance). Use ReLU as the activation function (torch.relu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ti8_83WzHP"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Any Pytorch's network class is an extension of the torch.nn.Module parent class.\n",
        "# To define a network class, you need to define at least 2 methods: an __init__() method (constructor) and a forward() method\n",
        "class SimpleNetwork(torch.nn.Module):\n",
        "    # Create the network class by filling in this block of code\n",
        "\n",
        "    # Create the constructor. Add any additional arguments as you wish\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # Define the feed forward function.\n",
        "    # x is the input example/examples.\n",
        "    # Add any additional arguments as you wish.\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONBn93ldWz8n"
      },
      "source": [
        "### Part c \n",
        "Train the network using the training dataset. Use the SGD optimizer and CrossEntropyLoss. After each epoch, record the current loss and the current training accuracy. The current training accuracy is obtained by evaluating the model on the train dataset. Use the DataLoaders defined in part a to efficiently pass training and testing data.\n",
        "\n",
        "You can learn about the available optimizers at:\n",
        "https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "You can learn about the available loss functions at:\n",
        "https://pytorch.org/docs/stable/nn.html#loss-functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zDhPrRYW4ho"
      },
      "source": [
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Train the network by filling in this block of code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31M2YA4W-yk"
      },
      "source": [
        "Plot how the loss and the training accuracy and the validation accuracy change over the epochs. Is there a point where overfitting occurs? If you cannot spot one, answer no. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiAD2opxW_gY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCRxAtjnXB55"
      },
      "source": [
        "### Part d \n",
        "Evaluate the model on the test dataset. Print out the accuracy. Does this accuracy agrees with the training accuracy showed on the plot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGAvfpWuXHLM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9NTStY73w"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "In this problem, we will investigate the effects of various common hyperparameters on the performance of a neural network. In the following cell, you can find a network class already defined for you. You can initiate network instances with different hyperparameters by changing the contructor's arguments.\n",
        "\n",
        "You are graded based on how you implement and execute the experiments. Since there is some randomness in initiating and training a neural network, there is no guarantee that you will get an expected result for an experiment or that your results should be similar to those of your peers. The expected outcome is that you execute the experiments correctly and the conclusion you get are consistent with your results. For each experiments, try to run the code multiple times and record the average results like what we did in Homework 2 (it will take some time to run, as expected when training any neural network)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gomk6vh0ZjQJ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class mnist_network(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, num_hidden_layers=1, layer_size = 100, activation=None):\n",
        "        super(mnist_network, self).__init__()\n",
        "        # layers of the network\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.layer_size = layer_size\n",
        "        self.activation = activation\n",
        "\n",
        "        if(self.activation is 'relu'):\n",
        "            self.activation = F.relu\n",
        "        elif(self.activation is 'tanh'):\n",
        "            self.activation = torch.tanh\n",
        "\n",
        "        self.layers = nn.ModuleList([nn.Linear(784,self.layer_size)])\n",
        "        for i in range(1, self.num_hidden_layers):\n",
        "            self.layers.append(nn.Linear(self.layer_size,self.layer_size))\n",
        "        self.layers.append(nn.Linear(self.layer_size,10))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # converting each image into a vector\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.reshape(batch_size,-1)\n",
        "        # rest of the forward pass \n",
        "        for i in range(self.num_hidden_layers+1):\n",
        "            x = self.layers[i](x)\n",
        "            if(self.activation is not None):\n",
        "                x = self.activation(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0DVPRosZkSF"
      },
      "source": [
        "Run the following code to load the MNIST dataset. For the sake of simplicity, we do not have a validation set in this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ8_gdWvZtpU"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "\n",
        "# DataLoader is a nice tool provided by PyTorch for passing training or testing examples\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=64)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCTzQwi5GdRk"
      },
      "source": [
        "Here is an example of training and testing a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaSecQQAoDHb"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import itertools\n",
        "\n",
        "def train(model, criterion, train_loader, optimizer, epoch):\n",
        "    # Turn the model to training mode (gradients will be calculated)\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # We have to call zero_grad() on the optimizer to remove gradients from the previous data pass.\n",
        "        # Otherwise, the gradients will be accumulated throughout many passes.\n",
        "        optimizer.zero_grad()\n",
        "        # Pass in the data and obtain the output.\n",
        "        # When you pass the data directly by calling model(data), the model will internally pass the data through the forward() function.\n",
        "        output = model(data)\n",
        "        # Compare the output and the ground truth and calculate the loss.\n",
        "        loss = criterion(output, target)\n",
        "        # From the calculated loss, call backward() to calculate the gradients for all the paramters in the network.\n",
        "        loss.backward()\n",
        "        # Update the parameters according to the gradients. \n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, criterion, test_loader):\n",
        "    # Turn the model to testing mode (gradients will not be calculated)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 5\n",
        "# Learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# Create the model\n",
        "model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
        "\n",
        "# Define the training and testing loss\n",
        "train_criterion = nn.CrossEntropyLoss()\n",
        "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# Define the optimizer\n",
        "# We have to specify the learning rate and the parameters that the optimizer should update during training.\n",
        "# In this case, we specify that the optimizer should update all the parameters from our model.\n",
        "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "        # Training\n",
        "        train(model, train_criterion, train_loader, optimizer, epoch)\n",
        "        # Testing\n",
        "        test(model, test_criterion, test_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HhSC9k1h1pi"
      },
      "source": [
        "# Conder these lines of code from above\n",
        "epochs = 5\n",
        "lr = 0.01\n",
        "model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
        "\n",
        "# What will you change for each part of the problem below?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnm5pgiLhBmM"
      },
      "source": [
        "## Part a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spGF1F_NZvt1"
      },
      "source": [
        "First, we will investigate the effect of varying the size of the hidden layer. Create 3 one-hidden-layer networks with the sizes of the hidden layers being 5, 20, 50, respectively. We will call these the 5-network, the 20-network, and the 50-network. All networks should use ReLU activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpgpUMfEaLGQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLDj0aMPaOP6"
      },
      "source": [
        "Train the 5-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFtFPtnOayqM"
      },
      "source": [
        "# Number of training epochs\n",
        "epochs = 10\n",
        "# Learning rate\n",
        "lr = 0.001\n",
        "\n",
        "# Create the model\n",
        "model = mnist_network(num_hidden_layers=1,layer_size=20,activation='relu')\n",
        "\n",
        "# Define the training and testing loss\n",
        "train_criterion = nn.CrossEntropyLoss()\n",
        "test_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# Define the optimizer\n",
        "# We have to specify the learning rate and the parameters that the optimizer should update during training.\n",
        "# In this case, we specify that the optimizer should update all the parameters from our model.\n",
        "optimizer = optim.SGD(model.parameters(),lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "        # Training\n",
        "        train(model, train_criterion, train_loader, optimizer, epoch)\n",
        "        # Testing\n",
        "        test(model, test_criterion, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShqrwsCjazpB"
      },
      "source": [
        "Test the trained 5-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUbHZYLxa5yw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b39B1xD9bhez"
      },
      "source": [
        "Train the 20-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKUZhq6lbfvB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqezVf-ybfVR"
      },
      "source": [
        "Test the trained 20-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JwFDxuubeki"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYFzQ_AObdp0"
      },
      "source": [
        "Train the 50-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpQiI7o-bzdv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnTUT4Gkb0D3"
      },
      "source": [
        "Test the trained 50-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcjuzPU-b7AU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JdznrIAb7jy"
      },
      "source": [
        "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrzcDMwhcOJP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmC8m-SRcOkP"
      },
      "source": [
        "What is your conclustion on the effect of varying the hidden layer size on the performance of a neural network trained on the MNIST dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdlMt7WdhE4b"
      },
      "source": [
        "## Part b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGBCmQHDa69o"
      },
      "source": [
        "Now, we will investigate the effect of varying the number of hidden layers. Create 3 networks with 1, 2, and 3 hidden layers, respectively. The size of all hidden layers should be 20 and the activation function is ReLU. We will call these the 1-network, the 2-network, and the 3-network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbZfJ7Abbc2P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9RMn8b9cm9i"
      },
      "source": [
        "Train the 1-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, \n",
        "\n",
        "---\n",
        "\n",
        "record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km6IknYlcm9j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHW3-J4vcm9k"
      },
      "source": [
        "Test the trained 1-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iROIK0lgcm9m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37h7g55ecm9n"
      },
      "source": [
        "Train the 2-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c7TL_5Icm9o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fJGIL1Hcm9p"
      },
      "source": [
        "Test the trained 2-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Sr0On6cm9p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3KteClMcm9q"
      },
      "source": [
        "Train the 3-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thWHY7rWcm9r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB4nTkYecm9s"
      },
      "source": [
        "Test the trained 3-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "176L8ou0cm9t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW0D0CWWcm9t"
      },
      "source": [
        "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW6JCxpmcm9t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65eqaTMbcm9u"
      },
      "source": [
        "What is your conclustion on the effect of varying the number of hidden layers on the performance of a neural network trained on the MNIST dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrIobY5hJe5"
      },
      "source": [
        "## Part c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_rfiw5c6ZL"
      },
      "source": [
        "Next, we will investigate the effects of varying the activation functions on a neural network. Create 3 networks. The first network has Sigmoid activation (Sigmoid-network). The second network has ReLU activation (ReLU-network). The third network has Tanh activation (Tanh-network). All networks have one hidden layer with size 20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vboiquSGc4cE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py5w8S3Cd9E0"
      },
      "source": [
        "Train the Sigmoid-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dGgpGwyd9E2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6PTSxcDd9E2"
      },
      "source": [
        "Test the trained Sigmoid-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKhk1nq7d9E3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hpbsbN1d9E4"
      },
      "source": [
        "Train the ReLU-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu3wbDf2d9E7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYOANVPWd9E7"
      },
      "source": [
        "Test the trained ReLU-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw_lcZAsd9E8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66X8ymnLd9E8"
      },
      "source": [
        "Train the Tanh-network on the MNIST dataset for 10 epochs with learning rate 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8HulP31d9E9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYt6o7Aed9E-"
      },
      "source": [
        "Test the trained Tanh-network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7-YK5gVd9E_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIGx8kPTd9FA"
      },
      "source": [
        "Plot the training accuracies over the epochs of the networks on the same figure (there should 3 line plots/scatter plots). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVy4iVnMd9FA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCGjP911d9FB"
      },
      "source": [
        "What is your conclustion on the effect of varying the activation functions on the performance of a neural network trained on MNIST dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiAZ36IdhMvG"
      },
      "source": [
        "## Part d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H1N3i-derH7"
      },
      "source": [
        "Finally, we will look into the effect of varying the value of the learning rate on the performance of a neural network. Create a network with one hidden layer of size 20 and ReLU activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKp-TYHlea9w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUFgUcGUfG7z"
      },
      "source": [
        "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to be 0.1. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtLuEUNKfG72"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC5CD3zmfG73"
      },
      "source": [
        "Test the trained network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT04G-BLfG74"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REVtrsiqfG75"
      },
      "source": [
        "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to be 0.01. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEXtS_XEfG76"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2dLcxgdfG77"
      },
      "source": [
        "Test the trained network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT9_RHk_fG78"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJu3jwdKfG79"
      },
      "source": [
        "Train the network on the MNIST dataset for 10 epochs. Set the learning rate to 0.001. After each epoch, record the current training accuracy of the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_GDwCxkfG7-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HbaGOWRfG7_"
      },
      "source": [
        "Test the trained network on the test data. Print out the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp6jgPocfG8A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAv1ujIfG8A"
      },
      "source": [
        "Plot the training accuracies over the epochs of the scenarios on the same figure (there should 3 line plots/scatter plots). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTazEXPkfG8B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLal-1hxfG8B"
      },
      "source": [
        "What is your conclustion on the effect of varying the learning rate on the performance of a neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9FJtJgufv-n"
      },
      "source": [
        "## REMARK for Problem 2\n",
        "\n",
        "You have observed the effects of varying different hyperparameters on the performance of a neural network **on the MNIST dataset**. However, keep in mind that these trends only apply for **the MNIST dataset** and should not be carried to another problem. There is no single hyperparameter settings that works for all problems. As you do more problems, you will build up your intuitions about the hyperparameters so that you can quickly deploy a good model. For example, people observed that setting the learning rate = 0.001 often works the best, though it is not always the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHSaycfVdHRJ"
      },
      "source": [
        "## Problem 3\n",
        "\n",
        "Experimenting with **k-anomity, i-diversity, and t-closeness**. \n",
        "\n",
        "Consider a dataset, for example, with 3 ordinary attributes and 1 sensitive attribute. Let the 3 ordinary attributes be Age, Sex, and Education and the sensitive attribute be Income, each row in this dataset is of the form:\n",
        "\n",
        "$$\n",
        "    [Age, Sex, Education, Income]\n",
        "$$\n",
        "\n",
        "A hacker is interested in knowing the sensitive attribute Income. When the dataset is designed so that if complies with either **k-anomity**, **i-diversity**, and/or **t-closeness**, even if he or she somehow figures out the values of the three, the hacker may not retrive the sensitive information accurately. In general, **k-anomity** is weaker than **i-diversity**, which, in turn, is weaker than **t-closeness**.\n",
        "\n",
        "By definition, **k-anomity** means that there is at least **k** different rows in the table of which ordinary values are a particular combination of Age, Sex, and Education. For example, the hacker knows the information of the person of interest is Age = 31, Sex = Female, and Education = BS. He or she looks into the data table and found that there are 3 rows with that combination:\n",
        "\n",
        "$$\n",
        "    [Age=31, Sex=Female, Education=BS, Income=300k]\n",
        "$$\n",
        "$$\n",
        "    [Age=31, Sex=Female, Education=BS, Income=70k]\n",
        "$$\n",
        "$$\n",
        "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
        "$$\n",
        "\n",
        "The hacker cannot tell accurately what the income of the person is because it can be one of the 3 values shown. This particular combination of information has 3-anomity. If every combination corresponds to at least 3 rows, then the dataset has 3-anomity.\n",
        "\n",
        "a) Let's look at the dataset **\"table.csv\"**. Let the sensitive attribute be **education** and others be ordinary attributes. Calculate the anomity of the dataset (the value **k**). First, find all the posible combinations of the ordinary attributes that exists in the dataset. After that, determine the anomity for each combination. The anomity of the dataset is the smallest anomity among the combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrm7S5y5dHRJ"
      },
      "source": [
        "# As you read here, k-anonyminity is calculated by looking at the combinations of rows / features  \n",
        "# not including the sensitive attribute\n",
        "# If we total up the number of times each combination occurs in a dataset, we can take\n",
        "# the minimum value of that and that will be our k-anonymity "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixGLowBjdHRK"
      },
      "source": [
        "We can improve the **k-anomity** of the dataset by \"suppressing\" the ordinary attributes. Suppressing means reducing the resolution of the attribute's value. For this problem, let's suppress Age by replacing the exact age with an age range. For example, instead of leaving age = 32, replace it with age = 30-40. Apply this to **\"table.csv\"** with the ranges {<20, 20-30, 30-50, >50}. Check if the anomity improves. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGqI5mltdHRK"
      },
      "source": [
        "# Due to the the number of possible combinations with all the unique values in the last part\n",
        "# the k-anonymity was probability pretty low, as you would need every combination to occur\n",
        "# at LEAST k times \n",
        "# So to make the k-anonymity higher, we can suppress some of the attributes to a range\n",
        "# to make the number of combinations lower and the number of each combination higher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv-RmXqCdHRK"
      },
      "source": [
        "**K-anomity** is nice, however, it fails in many cases. If the rows which share a combination of ordinary attributes have only a few values for the sensitive attribute, then it is not much better than having no anomity at all. For example, consider:\n",
        "\n",
        "$$\n",
        "    [Age=31, Sex=Female, Education=BS, Income=300k]\n",
        "$$\n",
        "$$\n",
        "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
        "$$\n",
        "$$\n",
        "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
        "$$\n",
        "$$\n",
        "    [Age=31, Sex=Female, Education=BS, Income=20k]\n",
        "$$\n",
        "\n",
        "When **k-anomity** fails in the second case, **i-diversity** comes to the rescue. **I-diversity** states that the rows of a particular combination of information must have at least i different values for the sensitive attribute. The above example has 2-diversity, which is not good. \n",
        "\n",
        "b) Calculate the **i-diversity** of the dataset **\"table.csv\"**. Follow similar steps as in part a. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZYCvcukdHRK"
      },
      "source": [
        "# K-anonymity does not really protect the material of the sensitive attribute itself though\n",
        "# I-diversity is the next step of security above that\n",
        "# If we look at all the combinations of attributes we generated from our k-anonymity \n",
        "# within each combination, we look at the different values of sensitive attributes\n",
        "# and count how mnay unique instances of it occur\n",
        "# If our i-diversity is low, ie 1, then the combination of other attributes does \n",
        "# not really protect the sensitive data, because if they know the other attributes,\n",
        "# then you essentially know the last value\n",
        "\n",
        "# To calculate i-diversity, you would go through all combinations of of your non-sensitive\n",
        "# attributes, and find the number of unique sensitive attributes\n",
        "# Then you would take the minimum number of this over the whole dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qWt9JvTdHRK"
      },
      "source": [
        "Suppressing an attribute can also improve the **i-diversity** of the dataset. Repeat the suppression as in **part a** and check if the diversity improves. If it does not, consider further suppress age by using the range {<20, 20-50, >50}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFVPKWrodHRK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX1lsm8ydHRK"
      },
      "source": [
        "**T-closeness** is even better than **i-diversity**. **T-closeness** requires that for every combination of information, the distribution of the sensitive attribute's value among the corresponding rows must be close to the overall distribution of the sensitive attribute's value for the whole dataset. Distance between distribution is calculated using the Earth Mover Distance (EMD). The dataset has **t-closeness** if no distance exceeds **t**. \n",
        "\n",
        "c) Calculate the overall distribution of **education**. Find the **t-closeness** of the dataset (largest distance between any combination's distribution of marital-status and the overall distribution).\n",
        "\n",
        "You can use **scipy.stats.wasserstein_distance** to calculate the EMD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjR4iG2bdHRK"
      },
      "source": [
        "# We just saw with i-diversity that we do not want just a few values for every combination\n",
        "# For every combination of attributes, we want to have a nice distribution \n",
        "# of sensitive values\n",
        "# And ideally, we want to have this distribution of sensitive information within each \n",
        "# combination match the distribution of sensitive values for the entire dataset\n",
        "\n",
        "# T-closeness measures how close the distribution of values within a combination is \n",
        "# to the distribution of the entire dataset\n",
        "\n",
        "# If we find the distribution of the whole dataset, and then loop through the distributions\n",
        "# of each combination, we can calculate the EMD (distance between distributions) \n",
        "# for each of the combinations\n",
        "EMD = wasserstein_distance(overall_distribution,distribution)\n",
        "\n",
        "# We are wanting to find the max distance in this problem, so we would take the \n",
        "# max value of this across all combinations "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}